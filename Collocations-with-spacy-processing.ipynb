{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.collocations import BigramAssocMeasures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../tf-idf-programming-historian/txt/0426.txt'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "all_txt_files =[]\n",
    "# get all file names for obits in txt folder\n",
    "for root, dirs, files in os.walk(\"txt\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "            all_txt_files.append(os.path.join(root, file))\n",
    "n_files = len(all_txt_files)\n",
    "all_txt_files[365]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of strings. Each string is an obit\n",
    "all_text = []\n",
    "for i in all_txt_files:\n",
    "    with open(i) as f:\n",
    "        txt = f.read()\n",
    "    all_text.append(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Late in the afternoon of Friday, Feb. 24, 1956, a short, rotund, round-headed, gleamingly bald, bagg'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text[0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# define a function to tokenize, lowercase all, remove punctuation, remove numbers etc.\n",
    "def text_cleanup(mystring):\n",
    "    x = ['SPACE', 'PUNCT', 'SYM', 'X', 'NUM']\n",
    "    doc = nlp(mystring.lower())\n",
    "    processed = [i.text for i in doc if i.pos_ not in x]\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code will take a long time to execute ... \n",
    "all_docs = [text_cleanup(i) for i in all_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['authentic',\n",
       " 'material',\n",
       " '$$$$',\n",
       " '$$$$',\n",
       " '$$$$',\n",
       " '$$$$',\n",
       " '$$$$',\n",
       " '$$$$',\n",
       " '$$$$',\n",
       " 'a',\n",
       " 'man',\n",
       " 'of',\n",
       " 'missionary']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_docs_merged = []\n",
    "for i in all_docs:\n",
    "    # insert seven instances of the string '$$$$' between each obit so collocations don't cross documents \n",
    "    all_docs_merged.extend(i)\n",
    "    all_docs_merged.extend(['$$$$','$$$$','$$$$','$$$$','$$$$','$$$$','$$$$'])\n",
    "    \n",
    "all_docs_merged[6537:6550]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(all_docs_merged[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate BigramAssocMeasures()\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "\n",
    "# initialize the bigram collocation finder object to find and rank collocations\n",
    "finder = BigramCollocationFinder.from_words(all_docs_merged, window_size=7)\n",
    "\n",
    "# consider only collocations for terms that appear at least three times in the corpus\n",
    "finder.apply_freq_filter(2)\n",
    "\n",
    "# list of all possible measures: .raw_freq, .pmi, .likelihood_ratio, .chi_sq, .phi_sq, .fisher, .student_t, .mi_like, .poisson_stirling, .jaccard, .dice\n",
    "results = finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15121"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = finder.score_ngrams(bigram_measures.pmi)\n",
    "\n",
    "#check length of scores\n",
    "len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this bit of code loops through the scores list and saves only results that include the seed word\n",
    "# in this case, the seed word is 'life' (seems like a good seed for obits)\n",
    "seed = \"life\"\n",
    "result = []\n",
    "for terms, score in scores:\n",
    "    if terms[0] == seed:\n",
    "        result.append([terms[0], terms[1], score])\n",
    "    if terms[1] == seed:   \n",
    "        result.append([terms[0], terms[1], score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['life', 'cycle', 5.797787832454242],\n",
       " ['life', 'span', 5.797787832454242],\n",
       " ['mold', 'life', 5.797787832454242],\n",
       " ['life', 'experiences', 5.212825331733087],\n",
       " ['middle', 'life', 4.4758597375668785],\n",
       " ['purpose', 'life', 4.4758597375668785],\n",
       " ['writing', 'life', 4.4758597375668785],\n",
       " ['stage', 'life', 4.3827503331754],\n",
       " ['&', 'life', 3.9904329103966383],\n",
       " ['fame', 'life', 3.9904329103966383]]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result is a list of lists; each list is a row of data\n",
    "result[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can now save a csv of my collocations for the seed word\n",
    "import pandas as pd\n",
    "df = pd.DataFrame.from_records(result, columns=[\"word1\", \"word2\", \"score\"])\n",
    "df.to_csv(\"collocations_life.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
